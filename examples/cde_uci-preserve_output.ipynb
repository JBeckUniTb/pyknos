{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain data and configure access\n",
    "Data should be downloaded from https://zenodo.org/record/1161203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# get initial path and data path, verify data path\n",
    "notebook_path = os.getcwd()\n",
    "\n",
    "data_path = os.path.expanduser('~/data/uci_data')\n",
    "assert os.path.isdir(data_path)\n",
    "\n",
    "best_state_dict_file = os.path.join(data_path, 'best_state_dict.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Options and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_batch_size = 64\n",
    "\n",
    "val_batch_size = 512\n",
    "\n",
    "learning_rate = 3e-4\n",
    "\n",
    "num_training_steps = int(2e5)\n",
    "\n",
    "anneal_learning_rate = True\n",
    "\n",
    "grad_norm_clip_value = 5.0\n",
    "\n",
    "#'affine-coupling', 'quadratic-coupling', 'rq-coupling', 'affine-autoregressive'\n",
    "#'quadratic-autoregressive', 'rq-autoregressive'\n",
    "base_transform_type = 'rq-autoregressive' \n",
    "\n",
    "# 'permutation', 'lu', 'svd'\n",
    "linear_transform_type = 'lu'  \n",
    "\n",
    "num_flow_steps = 10\n",
    "\n",
    "# Number of hidden features to use in coupling/autoregressive nets\n",
    "hidden_features = 256  \n",
    "\n",
    "tail_bound = 3  # Box is on [-bound, bound]^2\n",
    "\n",
    "num_bins = 8  # Number of bins to use for piecewise transforms.\n",
    "\n",
    "num_transform_blocks = 2  # Number of blocks to use in coupling/autoregressive nets\n",
    "\n",
    "use_batch_norm = False\n",
    "\n",
    "dropout_probability = 0.25  # Dropout probability for coupling/autoregressive nets\n",
    "\n",
    "monitor_interval = 250  # Interval in steps at which to report training stats.\n",
    "\n",
    "seed = 1638128  # Random seed for PyTorch and NumPy\n",
    "\n",
    "dtype = 'float32'  # data will be cast to this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.plot([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import outside packages\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils import data\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import from pyknos\n",
    "from pyknos.nflows import distributions, transforms, flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## options and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 64\n",
    "\n",
    "num_training_steps = int(2e5)\n",
    "\n",
    "anneal_learning_rate = True\n",
    "\n",
    "num_flow_steps = 10\n",
    "\n",
    "#'affine-coupling', 'quadratic-coupling', 'rq-coupling', 'affine-autoregressive'\n",
    "#'quadratic-autoregressive', 'rq-autoregressive'\n",
    "base_transform_type = 'rq-autoregressive' \n",
    "\n",
    "# 'permutation', 'lu', 'svd'\n",
    "linear_transform_type = 'lu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    torch.set_default_tensor_type('torch.cpu.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load miniboone data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on nsf/data/miniboone.py\n",
    "dataset_name = 'miniboone'\n",
    "\n",
    "#load data\n",
    "datafile = os.path.join(data_path, 'miniboone', 'data.npy')\n",
    "assert os.path.exists(datafile)\n",
    "data_all = np.load(datafile).astype(dtype)\n",
    "\n",
    "#divide into train, test, val sets\n",
    "N_test = int(0.1 * data_all.shape[0])\n",
    "data_test = data_all[-N_test:]\n",
    "tmp = data_all[0:-N_test]\n",
    "N_validate = int(0.1 * tmp.shape[0])\n",
    "data_validate = tmp[-N_validate:]\n",
    "data_train = tmp[0:-N_validate]\n",
    "\n",
    "#normalize\n",
    "tmp = np.vstack((data_train, data_validate))\n",
    "mu = tmp.mean(axis=0)\n",
    "s = tmp.std(axis=0)\n",
    "data_train = (data_train - mu) / s\n",
    "data_val = (data_validate - mu) / s\n",
    "data_test = (data_test - mu) / s\n",
    "\n",
    "#create data loaders\n",
    "train_loader = data.DataLoader(dataset=data_train, batch_size=train_batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = data.DataLoader(dataset=data_val, batch_size=train_batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = data.DataLoader(dataset=data_test, batch_size=train_batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "def batch_generator(loader, num_batches=int(1e10)):\n",
    "    batch_counter = 0\n",
    "    while True:\n",
    "        for batch in loader:\n",
    "            yield batch\n",
    "            batch_counter += 1\n",
    "            if batch_counter == num_batches:\n",
    "                return\n",
    "train_generator = batch_generator(train_loader)\n",
    "test_batch = next(iter(train_loader)).to(device)\n",
    "\n",
    "assert data_train.ndim == 2\n",
    "features = data_train.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define convenience functions for creating the flow transform based on the above settings\n",
    "def create_linear_transform():\n",
    "    if linear_transform_type == 'permutation':\n",
    "        return transforms.RandomPermutation(features=features)\n",
    "    elif linear_transform_type == 'lu':\n",
    "        return transforms.CompositeTransform([\n",
    "            transforms.RandomPermutation(features=features),\n",
    "            transforms.LULinear(features, identity_init=True)\n",
    "        ])\n",
    "    elif linear_transform_type == 'svd':\n",
    "        return transforms.CompositeTransform([\n",
    "            transforms.RandomPermutation(features=features),\n",
    "            transforms.SVDLinear(features, num_householder=10, identity_init=True)\n",
    "        ])\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "\n",
    "def create_base_transform(i):\n",
    "    if base_transform_type == 'affine-coupling':\n",
    "        return transforms.AffineCouplingTransform(\n",
    "            mask=utils.create_alternating_binary_mask(features, even=(i % 2 == 0)),\n",
    "            transform_net_create_fn=lambda in_features, out_features: nn_.ResidualNet(\n",
    "                in_features=in_features,\n",
    "                out_features=out_features,\n",
    "                hidden_features=hidden_features,\n",
    "                context_features=None,\n",
    "                num_blocks=num_transform_blocks,\n",
    "                activation=F.relu,\n",
    "                dropout_probability=dropout_probability,\n",
    "                use_batch_norm=use_batch_norm\n",
    "            )\n",
    "        )\n",
    "    elif base_transform_type == 'quadratic-coupling':\n",
    "        return transforms.PiecewiseQuadraticCouplingTransform(\n",
    "            mask=utils.create_alternating_binary_mask(features, even=(i % 2 == 0)),\n",
    "            transform_net_create_fn=lambda in_features, out_features: nn_.ResidualNet(\n",
    "                in_features=in_features,\n",
    "                out_features=out_features,\n",
    "                hidden_features=hidden_features,\n",
    "                context_features=None,\n",
    "                num_blocks=num_transform_blocks,\n",
    "                activation=F.relu,\n",
    "                dropout_probability=dropout_probability,\n",
    "                use_batch_norm=use_batch_norm\n",
    "            ),\n",
    "            num_bins=num_bins,\n",
    "            tails='linear',\n",
    "            tail_bound=tail_bound,\n",
    "            apply_unconditional_transform=apply_unconditional_transform\n",
    "        )\n",
    "    elif base_transform_type == 'rq-coupling':\n",
    "        return transforms.PiecewiseRationalQuadraticCouplingTransform(\n",
    "            mask=utils.create_alternating_binary_mask(features, even=(i % 2 == 0)),\n",
    "            transform_net_create_fn=lambda in_features, out_features: nn_.ResidualNet(\n",
    "                in_features=in_features,\n",
    "                out_features=out_features,\n",
    "                hidden_features=hidden_features,\n",
    "                context_features=None,\n",
    "                num_blocks=num_transform_blocks,\n",
    "                activation=F.relu,\n",
    "                dropout_probability=dropout_probability,\n",
    "                use_batch_norm=use_batch_norm\n",
    "            ),\n",
    "            num_bins=num_bins,\n",
    "            tails='linear',\n",
    "            tail_bound=tail_bound,\n",
    "            apply_unconditional_transform=apply_unconditional_transform\n",
    "        )\n",
    "    elif base_transform_type == 'affine-autoregressive':\n",
    "        return transforms.MaskedAffineAutoregressiveTransform(\n",
    "            features=features,\n",
    "            hidden_features=hidden_features,\n",
    "            context_features=None,\n",
    "            num_blocks=num_transform_blocks,\n",
    "            use_residual_blocks=True,\n",
    "            random_mask=False,\n",
    "            activation=F.relu,\n",
    "            dropout_probability=dropout_probability,\n",
    "            use_batch_norm=use_batch_norm\n",
    "        )\n",
    "    elif base_transform_type == 'quadratic-autoregressive':\n",
    "        return transforms.MaskedPiecewiseQuadraticAutoregressiveTransform(\n",
    "            features=features,\n",
    "            hidden_features=hidden_features,\n",
    "            context_features=None,\n",
    "            num_bins=num_bins,\n",
    "            tails='linear',\n",
    "            tail_bound=tail_bound,\n",
    "            num_blocks=num_transform_blocks,\n",
    "            use_residual_blocks=True,\n",
    "            random_mask=False,\n",
    "            activation=F.relu,\n",
    "            dropout_probability=dropout_probability,\n",
    "            use_batch_norm=use_batch_norm\n",
    "        )\n",
    "    elif base_transform_type == 'rq-autoregressive':\n",
    "        return transforms.MaskedPiecewiseRationalQuadraticAutoregressiveTransform(\n",
    "            features=features,\n",
    "            hidden_features=hidden_features,\n",
    "            context_features=None,\n",
    "            num_bins=num_bins,\n",
    "            tails='linear',\n",
    "            tail_bound=tail_bound,\n",
    "            num_blocks=num_transform_blocks,\n",
    "            use_residual_blocks=True,\n",
    "            random_mask=False,\n",
    "            activation=F.relu,\n",
    "            dropout_probability=dropout_probability,\n",
    "            use_batch_norm=use_batch_norm\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "\n",
    "def create_transform():\n",
    "    transform = transforms.CompositeTransform([\n",
    "        transforms.CompositeTransform([\n",
    "            create_linear_transform(),\n",
    "            create_base_transform(i)\n",
    "        ]) for i in range(num_flow_steps)\n",
    "    ] + [\n",
    "        create_linear_transform()\n",
    "    ])\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the flow\n",
    "distribution = distributions.StandardNormal((features,))\n",
    "transform = create_transform()\n",
    "flow = flows.Flow(transform, distribution).to(device)\n",
    "\n",
    "n_params = utils.get_num_parameters(flow)\n",
    "print('There are {} trainable parameters in this model.'.format(n_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create optimizer\n",
    "optimizer = optim.Adam(flow.parameters(), lr=learning_rate)\n",
    "if anneal_learning_rate:\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, num_training_steps, 0)\n",
    "else:\n",
    "    scheduler = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbar = tqdm(range(num_training_steps))\n",
    "best_val_score = -1e10\n",
    "\n",
    "#loss_vals = np.full(num_training_steps, np.nan)\n",
    "\n",
    "val_densities = []\n",
    "\n",
    "for step in tbar:\n",
    "    \n",
    "    flow.train()  # set flow into training mode\n",
    "    \n",
    "    if anneal_learning_rate:\n",
    "        scheduler.step(step)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    batch = next(train_generator).to(device)\n",
    "    log_density = flow.log_prob(batch)\n",
    "    loss = - torch.mean(log_density)\n",
    "    loss.backward()  # calculate gradients\n",
    "    \n",
    "    if grad_norm_clip_value is not None:\n",
    "        clip_grad_norm_(flow.parameters(), grad_norm_clip_value)\n",
    "        \n",
    "    optimizer.step()\n",
    "\n",
    "    #writer.add_scalar(tag='loss', scalar_value=loss.item(), global_step=step)\n",
    "\n",
    "    if (step + 1) % monitor_interval == 0:\n",
    "        flow.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # compute validation score\n",
    "            running_val_log_density = 0\n",
    "            for val_batch in val_loader:\n",
    "                log_density_val = flow.log_prob(val_batch.to(device).detach())\n",
    "                mean_log_density_val = torch.mean(log_density_val).detach()\n",
    "                running_val_log_density += mean_log_density_val\n",
    "            running_val_log_density /= len(val_loader)\n",
    "\n",
    "        val_densities.append(running_val_log_density.item())\n",
    "            \n",
    "        if running_val_log_density > best_val_score:\n",
    "            best_val_score = running_val_log_density\n",
    "            #path = os.path.join(cutils.get_checkpoint_root(), '{}-best-val-{}.t'.format(dataset_name, timestamp))\n",
    "            torch.save(flow.state_dict(), best_state_dict_file)\n",
    "\n",
    "        # compute reconstruction\n",
    "        with torch.no_grad():\n",
    "            test_batch_noise = flow.transform_to_noise(test_batch)\n",
    "            test_batch_reconstructed, _ = flow._transform.inverse(test_batch_noise)\n",
    "        errors = test_batch - test_batch_reconstructed\n",
    "        max_abs_relative_error = torch.abs(errors / test_batch).max()\n",
    "        average_abs_relative_error = torch.abs(errors / test_batch).mean()\n",
    "        #writer.add_scalar('max-abs-relative-error', max_abs_relative_error, global_step=step)\n",
    "        #writer.add_scalar('average-abs-relative-error', average_abs_relative_error, global_step=step)\n",
    "\n",
    "        summaries = {\n",
    "            'val': running_val_log_density.item(),\n",
    "            'best-val': best_val_score.item(),\n",
    "            'max-abs-relative-error': max_abs_relative_error.item(),\n",
    "            'average-abs-relative-error': average_abs_relative_error.item()\n",
    "        }\n",
    "        #for summary, value in summaries.items():\n",
    "        #    writer.add_scalar(tag=summary, scalar_value=value, global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best val model\n",
    "#path = os.path.join(cutils.get_checkpoint_root(),\n",
    "#                    '{}-best-val-{}.t'.format(dataset_name, timestamp))\n",
    "flow.load_state_dict(torch.load(best_state_dict_file))\n",
    "flow.eval()  # put flow in evaluation mode\n",
    "\n",
    "# calculate log-likelihood on test set\n",
    "with torch.no_grad():\n",
    "    log_likelihood = torch.Tensor([])\n",
    "    for batch in tqdm(test_loader):\n",
    "        log_density = flow.log_prob(batch.to(device))\n",
    "        log_likelihood = torch.cat([\n",
    "            log_likelihood,\n",
    "            log_density\n",
    "        ])\n",
    "#path = os.path.join(log_dir, '{}-{}-log-likelihood.npy'.format(\n",
    "#    dataset_name,\n",
    "#    base_transform_type\n",
    "#))\n",
    "#np.save(path, utils.tensor2numpy(log_likelihood))\n",
    "mean_log_likelihood = log_likelihood.mean()\n",
    "std_log_likelihood = log_likelihood.std()\n",
    "\n",
    "# save log-likelihood\n",
    "s = 'Final score for {}: {:.2f} +- {:.2f}'.format(\n",
    "    dataset_name.capitalize(),\n",
    "    mean_log_likelihood.item(),\n",
    "    2 * std_log_likelihood.item() / np.sqrt(len(data_test))\n",
    ")\n",
    "print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
